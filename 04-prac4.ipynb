{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIT 112 - Data Science Concepts \n",
    "\n",
    "\n",
    "## Tutorial 4\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "* become familiar with numpy package\n",
    "    * basic representation/transformation of vectors, matrices \n",
    "    * compute distances such as Euclidean distance and cosine distance\n",
    "* construct term-by-document matrix from text data\n",
    "* apply tf-idf\n",
    "* retrieve a document\n",
    "* plot a tagcloud\n",
    "    * in form of histogram with label\n",
    "    * in form of word visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Distance between 2 vectors\n",
    "\n",
    "`Distance` is a numerical description of how far apart objects are. It is a concrete way of describing what it means for elements of some space to be close or far away from each other. For example the distance between two vectors in an n-dimensional space.\n",
    "\n",
    "Now that you have know how to represent an n-dimensional vector in Python with NumPy arrays, we will write a function as a metric to measure the distance between two vectors. There are multiple ways to measure the distance between two vectors. We will discuss Euclidean distance and cosine distance.\n",
    "\n",
    "#####Euclidean distance\n",
    "\n",
    "Euclidean distance comes from Geometry. If we assume $\\mathbf{x}_{1}=\\left[x_{11},x_{12},\\ldots,x_{1n}\\right]$ and $\\mathbf{x}_{2}=\\left[x_{21},x_{22},\\ldots,x_{2n}\\right]$, then the Euclidean distance between $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ is defined as:\n",
    "\n",
    "$$d\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)=\\sqrt{\\left(x_{11}-x_{21}\\right)^{2}+\\left(x_{12}-x_{22}\\right)^{2}+\\ldots+\\left(x_{1n}-x_{2n}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "We can use array operators for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance1(x1, x2):\n",
    "    d = x1 - x2\n",
    "    d = d ** 2\n",
    "    return np.sqrt(d.sum())\n",
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "euclidean_distance1(x1, x2)\n",
    "Since two vectors passed to the function should be the same size, it is better to perform a sanity check before applying the subtraction. Otherwise it will raise an error. We can do this by using `if - elif` statement or as a better practice by using `try - except`.\n",
    "import sys\n",
    "\n",
    "def euclidean_distance2(x1, x2):\n",
    "    if x1.ndim != 1 or x2.ndim != 1:\n",
    "        sys.exit('either x1 or x2 or both are not vectors')\n",
    "    elif x1.shape[0] != x2.shape[0]:\n",
    "        sys.exit('x1 and x2 are not the same size')\n",
    "    else:\n",
    "        d = x1 - x2\n",
    "        d = d ** 2\n",
    "        return np.sqrt(d.sum())\n",
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "euclidean_distance2(x1, x2)\n",
    "def euclidean_distance3(x1, x2):\n",
    "    try:\n",
    "        d = x1 - x2\n",
    "        d = d ** 2\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        # you can return a default value\n",
    "        return None\n",
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "euclidean_distance3(x1, x2)\n",
    "#####cosine distance and cosine similarity\n",
    "Cosine similarity is a measure of similarity between two vectors that measures the cosine of the angle between them. Cosine similarity is widely used in information retrieval and text mining as a measure of similarity between documents and is defined as:\n",
    "\n",
    "$$S_{c}\\left(\\mathbf{x}_{1},\\mathbf{x_{2}}\\right)=\\frac{\\mathbf{x}_{1}.\\mathbf{x_{2}}}{\\parallel\\mathbf{x}_{1}\\parallel^{2}+\\parallel\\mathbf{x}_{2}\\parallel^{2}-\\mathbf{x}_{1}.\\mathbf{x_{2}}}$$\n",
    "\n",
    "\n",
    "Cosine similarity is particularly used in positive space where the outcome is bounded in [0, 1]. The cosine distance is defined as the complement to cosine similrity in positive space that is $D_{c}\\left(x_{1},x_{2}\\right)=1-S_{c}\\left(x_1,x_2\\right)$ where $D_c$ is the cosine distance and $S_c$ is the cosine similarity.\n",
    "def cosine_distance(x1, x2):\n",
    "    try:\n",
    "        num = (x1*x2).sum()\n",
    "        denom = (x1*x1).sum() + (x2*x2).sum() - (x1*x2).sum()\n",
    "        num += 0.0    # or use np.astype(float) to make sure of float division\n",
    "        return 1 - num/denom\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        return None\n",
    "x1 = np.array([2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "cosine_distance(x1, x2)\n",
    "np.divide(4, 5)\n",
    "###Term-by-Document matrix\n",
    "A term-by-document matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occure in the document collection. Each row corresponds to a document and each column correspond to a term. Thus the value that appears in row $j$ and column $i$ represents the frequency of appearing term $i$ in document $j$. (possibly link it to bag of words?)\n",
    "\n",
    "We will represent two datasets with term-by-document matrix:\n",
    "\n",
    "* a collection of 100 Twitter messages about Geelong\n",
    "* a collection of 6 news articles (5 about Apple and 1 about poletics)\n",
    "\n",
    "The data is already collected and stores in text files. Thus you will need to:\n",
    "\n",
    "* read the text files\n",
    "    * using file opbjetc\n",
    "* perform pre-processing\n",
    "    * using string methods\n",
    "    * using re package\n",
    "* construct the term-by-document matrix\n",
    "    * using numpy arrays and operations\n",
    "####Twitter dataset\n",
    "First read the data:\n",
    "import os\n",
    "\n",
    "# get current working directory\n",
    "cwd = os.getcwd()   \n",
    "\n",
    "# join the subdirectory of the data and data file name\n",
    "file_path = os.path.join(cwd, \"data\\\\prac04\\\\tweets.txt\")\n",
    "\n",
    "# read the contents of the file and store it in a list\n",
    "with open(file_path) as fp:\n",
    "    tweets = fp.readlines()    \n",
    "for tweet in tweets:\n",
    "    print tweet\n",
    "Mostly when dealing with data, we have to perform some sort of data pre-processing. Data collection is often loosely controlled, resulting in out of the range values, missing values, and etc. Thus quality of the data is first and formost before running an analysis. This step is specific to the nature of the data. For example for text data it may consist of cleaning, normalization, tokenization, and etc. \n",
    "\n",
    "In this case, our pre-processing consists of:\n",
    "\n",
    "* converting all the words into lower case to remove the effect of the letter case\n",
    "* replacing the URLs with a simple string such as 'url'. From the previous cell, you should be able to see that many of the tweets contain a URL. Since we are not using them now, we can remove them or replace them.\n",
    "* Removing the punctuations\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "def pre_process(doc):\n",
    "    \"\"\"\n",
    "    pre-processes a doc\n",
    "      * Converts the tweet into lower case,\n",
    "      * removes the URLs,\n",
    "      * removes the punctuations\n",
    "      * tokenizes the tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    # gettign rid of non ascii codes\n",
    "    doc = doc.decode('ascii', 'ignore')\n",
    "    \n",
    "    # repalcing URLs\n",
    "    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n",
    "    doc = re.sub(url_pattern, 'url', doc) \n",
    "\n",
    "    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|_|,|=|;|>|<|\\.\"\n",
    "    doc = re.sub(punctuation, ' ', doc)\n",
    "    \n",
    "    return doc.split()\n",
    "def termdoc(docs):\n",
    "    \"\"\"\n",
    "    returns the term-by-document matrix and the vocabulary of the passed corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()   \n",
    "    termdoc_sparse = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # pre-process the doc\n",
    "        doc_tokens = pre_process(doc)\n",
    "        # computes the frequencies for doc\n",
    "        doc_sparse = Counter(doc_tokens)    \n",
    "\n",
    "        termdoc_sparse.append(doc_sparse)\n",
    "\n",
    "        # update the vocab\n",
    "        vocab.update(doc_sparse.iterkeys())  \n",
    "\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    n_vocab = len(vocab)\n",
    "    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n",
    "\n",
    "    for j, doc_sparse in enumerate(termdoc_sparse):\n",
    "        for term, freq in doc_sparse.iteritems():\n",
    "            termdoc_dense[j, vocab.index(term)] = freq\n",
    "            \n",
    "    return termdoc_dense, vocab\n",
    "tweets_termdoc, tweets_vocab = termdoc(tweets)\n",
    "Lets look at one of tweets:\n",
    "j = 0\n",
    "print tweets[j]\n",
    "print tweets_termdoc[j]\n",
    "So baiscally, now each tweet is represented by a vector of size `len(tweets_vocab)`.\n",
    "####News dataset\n",
    "Similar to previous sections, the data is stored in text files names as'news1.txt', ..., 'news5.txt'. All we have to do is read the files, construct the corpus and send it to `termdoc()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_docs = 6\n",
    "cwd = os.getcwd()   \n",
    "news = []\n",
    "\n",
    "for j in xrange(1, n_docs+1):\n",
    "    filename = \"news{}.txt\".format(j)\n",
    "    file_path = os.path.join(cwd, \"data\\\\prac04\\\\{}\".format(filename))\n",
    "    with open(file_path) as fp:\n",
    "        news.append(fp.read())\n",
    "\n",
    "news_termdoc, news_vocab = termdoc(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now each news article is represented with a large vector of size `len(news_vocab)`. We can do many things with this representation. For example measuring the distance between two documents. The first 5 news articles are tech news and about Apple, but the 6th one is about poletics. We expect that tech news be more similar to each other rather than to the poletics news. In other words the distance between two articles frmom tech news should be less than the distance between a tech news article and a news article about politics. This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cosine_distance(news_termdoc[1], news_termdoc[2])   # both aout Apple\n",
    "print cosine_distance(news_termdoc[1], news_termdoc[5])   # one from tech world, the other from politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###TF-IDF\n",
    "\n",
    "Term count can reflect the importance of a term in a document. However, it does not consider the effect of document length and how popular the term is across multiple documents in the corpus. That's where **TF-IDF** comes in. It stands for \"Term Frequency - Inverse Document Frequency\" and it is a way to score the importance of words in document collection. Its concept is very simple and based on two intuitive rules:\n",
    " \n",
    "* if a word appears frequently in a documnt, it is important => Give it a high score\n",
    "* if a word appears in many documents, it is not a unique identifier => give it a low score\n",
    "\n",
    "Therefore words such as \"the\", \"for\", and \"of\" which appear in many documents are scored down. They are usually called \"stop-words\" and since they do not carry any significance, they are removed from the vocabulary. \n",
    "\n",
    "There are variations in how we calculate TF. For example raw frequency or log normalization. We will use the frequency which is equal to term count divided by the document length. There are also different ways to calculate IDF. We will use smooth inverse frequency which is defined as\n",
    "$$\\text{IDF}\\left(t,D\\right)=\\log\\left(\\frac{N_{D}}{1+|d\\in D\\,:\\, t\\in d|}\\right)$$\n",
    "\n",
    "Then TF-IDF of a term in a document in a corpus is calculated as TF of the term in the document multipled by the IDF of the word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(termdoc):\n",
    "    \"\"\"\n",
    "    returns the TF-IDF of a etrm by document matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    n_docs = termdoc.shape[0]\n",
    "    n_vocab = termdoc.shape[1]\n",
    "    \n",
    "    denom = (termdoc != 0).astype(int)\n",
    "    denom = 1.0 + denom.sum(axis=0)       \n",
    "    idf = np.log(n_docs / denom)\n",
    "    \n",
    "    row_sums = termdoc.sum(axis=1).astype(float)\n",
    "    tf = termdoc / row_sums[:, np.newaxis]\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_tf_idf = tf_idf(tweets_termdoc)\n",
    "news_tf_idf = tf_idf(news_termdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cosine distance for TF-IDF representation of the corpus\n",
    "\n",
    "print cosine_distance(news_tf_idf[1], news_tf_idf[2])   # both aout Apple\n",
    "print cosine_distance(news_tf_idf[1], news_tf_idf[5])   # one from tech world, the other from politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we write two functions to visualize a document. One using a histogram and the other using a WordCloud. The latter is a visual representation of text data where the importance of each term is shown by its font size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Histogram\n",
    "The package we use for plotting is called `matplotlib`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure figures are shown in ipython notebook\n",
    "%matplotlib inline    \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_hist(doc, vocab, K=25):\n",
    "    \n",
    "    if type(vocab) != np.ndarray:\n",
    "        vocab = np.array(vocab)\n",
    "    \n",
    "    # sorting the indices from max to min\n",
    "    idx = doc.argsort()[::-1][:K]\n",
    "    \n",
    "    x = np.arange(K)\n",
    "    y = doc[idx]\n",
    "    labels = vocab[idx]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    ax.bar(x, y)\n",
    "    ax.set_xticks(x+0.5)\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "    ax.set_xlabel('Terms')\n",
    "    ax.set_ylabel('TF-IDF weight')\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the histogram of any document in the corpus. For example the first news article. First we plot its count  representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(news_termdoc[0], news_vocab, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, since we have not removed the stopwords, the histogram is mostly showing meaningless words. We can not understand what the news is about by looking at the histogram. One remedy is removing stopwords and not using them in the vocabulary. Another remedy is using TF-IDF representation for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(news_tf_idf[0], news_vocab, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the histogram shows, the first news is about a 'streaming' 'service' to listen to 'songs' o 'phones' that 'connects' 'fans' to 'artists'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = news_tf_idf[0]\n",
    "vocab = news_vocab\n",
    "K = 50\n",
    "width = 500\n",
    "height = 500\n",
    "prefer_horiz = 0.9\n",
    "\n",
    "\n",
    "if type(vocab) != np.ndarray:\n",
    "    vocab = np.array(vocab)\n",
    "\n",
    "# sorting the indices from max to min\n",
    "idx = doc.argsort()[::-1][:K]\n",
    "\n",
    "freqs = doc[idx]\n",
    "freqs /= freqs.sum()\n",
    "\n",
    "word_pairs = [(vocab[idx[i]], freqs[i]) for i in range(K)]\n",
    "elements = wordcloud.fit_words(word_pairs, width=width, height=height,\n",
    "                                   prefer_horiz=prefer_horiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wordcloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
